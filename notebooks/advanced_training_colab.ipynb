{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced AI Text Detection - Complete Training Pipeline (Google Colab)\n",
        "\n",
        "This notebook is optimized for Google Colab and implements a comprehensive training pipeline for AI text detection, optimized to achieve 99%+ accuracy.\n",
        "\n",
        "**Features:**\n",
        "- All utilities included inline (no external files needed)\n",
        "- File upload widget for dataset\n",
        "- Optimized feature engineering (word + character n-grams)\n",
        "- Extensive hyperparameter tuning\n",
        "- Cross-validation for robust evaluation\n",
        "- Multiple model comparison\n",
        "- Comprehensive visualizations\n",
        "- Model download functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (Colab may not have all)\n",
        "%pip install -q scikit-learn scipy pandas matplotlib seaborn\n",
        "\n",
        "# Check if running on GPU instance (better CPU performance)\n",
        "import os\n",
        "if 'COLAB_GPU' in os.environ or os.environ.get('CUDA_VISIBLE_DEVICES'):\n",
        "    print(\"‚úì Running on GPU instance (better CPU performance)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Running on CPU - Consider switching to T4 GPU for faster training\")\n",
        "\n",
        "print(\"‚úì Packages installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from collections import Counter\n",
        "from time import time\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, \n",
        "    GridSearchCV, \n",
        "    cross_val_score,\n",
        "    StratifiedKFold,\n",
        "    learning_curve\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(\"‚úì All libraries imported successfully!\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions (Included for Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Include all utility functions inline for Colab\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Advanced text cleaning for AI detection.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    \n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Remove non-breaking spaces and special unicode\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    text = text.replace('\\u200b', '')\n",
        "    text = text.replace('\\u200c', '')\n",
        "    text = text.replace('\\u200d', '')\n",
        "    \n",
        "    # Remove excessive punctuation\n",
        "    text = re.sub(r'[!]{2,}', '!', text)\n",
        "    text = re.sub(r'[?]{2,}', '?', text)\n",
        "    text = re.sub(r'[.]{3,}', '...', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "print(\"‚úì Utility functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Upload and Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload dataset file\n",
        "print(\"=\" * 70)\n",
        "print(\"UPLOAD DATASET\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Please upload your 'combined_dataset_clean.json' file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"\\n‚úì File uploaded: {filename}\")\n",
        "    \n",
        "    # Load the dataset\n",
        "    dataset = json.loads(uploaded[filename].decode('utf-8'))\n",
        "    print(f\"‚úì Loaded {len(dataset):,} articles\")\n",
        "    \n",
        "    # Display sample\n",
        "    if len(dataset) > 0:\n",
        "        print(\"\\nSample article:\")\n",
        "        print(json.dumps(dataset[0], indent=2, ensure_ascii=False))\n",
        "else:\n",
        "    raise ValueError(\"No file uploaded! Please upload combined_dataset_clean.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze dataset distribution\n",
        "labels = [article.get(\"label\", \"\").lower() for article in dataset]\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATASET ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "for label, count in label_counts.items():\n",
        "    percentage = (count / len(dataset)) * 100\n",
        "    print(f\"  {label.upper()}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate text statistics\n",
        "text_lengths = []\n",
        "word_counts = []\n",
        "for article in dataset:\n",
        "    content = article.get(\"content\", \"\")\n",
        "    text_lengths.append(len(content))\n",
        "    word_counts.append(len(content.split()))\n",
        "\n",
        "print(f\"\\nText Statistics:\")\n",
        "print(f\"  Average length: {np.mean(text_lengths):.0f} characters\")\n",
        "print(f\"  Average words: {np.mean(word_counts):.0f} words\")\n",
        "print(f\"  Min length: {np.min(text_lengths)} characters\")\n",
        "print(f\"  Max length: {np.max(text_lengths):,} characters\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].bar(label_counts.keys(), label_counts.values(), color=['#3498db', '#e74c3c'])\n",
        "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Label')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "axes[1].hist(text_lengths, bins=50, color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "axes[1].set_title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Character Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "axes[2].hist(word_counts, bins=50, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
        "axes[2].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
        "axes[2].set_xlabel('Word Count')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess dataset\n",
        "print(\"=\" * 70)\n",
        "print(\"PREPROCESSING DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "processed_articles = []\n",
        "empty_count = 0\n",
        "\n",
        "for article in dataset:\n",
        "    label = article.get(\"label\", \"\").lower()\n",
        "    if label not in [\"ai\", \"human\"]:\n",
        "        continue\n",
        "    \n",
        "    content = article.get(\"content\", \"\").strip()\n",
        "    if not content:\n",
        "        empty_count += 1\n",
        "        continue\n",
        "    \n",
        "    cleaned_content = clean_text(content)\n",
        "    \n",
        "    if cleaned_content and len(cleaned_content) > 20:\n",
        "        processed_articles.append({\n",
        "            \"content\": cleaned_content,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "print(f\"‚úì Processed {len(processed_articles):,} articles\")\n",
        "print(f\"  Removed {len(dataset) - len(processed_articles):,} invalid/empty articles\")\n",
        "\n",
        "# Balance dataset\n",
        "ai_articles = [a for a in processed_articles if a[\"label\"] == \"ai\"]\n",
        "human_articles = [a for a in processed_articles if a[\"label\"] == \"human\"]\n",
        "\n",
        "print(f\"\\nBefore balancing:\")\n",
        "print(f\"  AI articles: {len(ai_articles):,}\")\n",
        "print(f\"  Human articles: {len(human_articles):,}\")\n",
        "\n",
        "min_size = min(len(ai_articles), len(human_articles))\n",
        "ai_articles = ai_articles[:min_size]\n",
        "human_articles = human_articles[:min_size]\n",
        "\n",
        "balanced_dataset = ai_articles + human_articles\n",
        "balanced_dataset = shuffle(balanced_dataset, random_state=RANDOM_STATE)\n",
        "\n",
        "print(f\"\\nAfter balancing:\")\n",
        "print(f\"  AI articles: {len(ai_articles):,}\")\n",
        "print(f\"  Human articles: {len(human_articles):,}\")\n",
        "print(f\"  Total: {len(balanced_dataset):,}\")\n",
        "\n",
        "texts = [article[\"content\"] for article in balanced_dataset]\n",
        "labels = [article[\"label\"] for article in balanced_dataset]\n",
        "\n",
        "if len(texts) == 0:\n",
        "    raise ValueError(\"No valid texts after preprocessing!\")\n",
        "\n",
        "print(f\"\\n‚úì Final dataset: {len(texts):,} samples ready for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode labels and split data\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nLabel encoding:\")\n",
        "print(f\"  Classes: {label_encoder.classes_}\")\n",
        "print(f\"  Distribution: AI={np.sum(y==1):,}, Human={np.sum(y==0):,}\")\n",
        "\n",
        "# Split data\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    texts, y, \n",
        "    test_size=0.2, \n",
        "    random_state=RANDOM_STATE, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain/Test Split:\")\n",
        "print(f\"  Training: {len(X_train_texts):,} samples\")\n",
        "print(f\"  Testing: {len(X_test_texts):,} samples\")\n",
        "\n",
        "# Word-level features (optimized for speed while maintaining accuracy)\n",
        "print(\"\\nCreating word-level features...\")\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    max_features=20000,  # Reduced from 25000 for faster processing\n",
        "    ngram_range=(1, 3),\n",
        "    sublinear_tf=True,\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    analyzer='word',\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode',\n",
        "    token_pattern=r'(?u)\\b\\w+\\b'\n",
        ")\n",
        "\n",
        "start_time = time()\n",
        "X_train_word = word_vectorizer.fit_transform(X_train_texts)\n",
        "X_test_word = word_vectorizer.transform(X_test_texts)\n",
        "print(f\"‚úì Word features created in {time() - start_time:.2f} seconds\")\n",
        "print(f\"  Training shape: {X_train_word.shape}\")\n",
        "print(f\"  Vocabulary size: {len(word_vectorizer.vocabulary_):,}\")\n",
        "\n",
        "# Character-level features (optimized for speed while maintaining accuracy)\n",
        "print(\"\\nCreating character-level features...\")\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3, 6),\n",
        "    max_features=30000,  # Reduced from 35000 for faster processing\n",
        "    sublinear_tf=True,\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode'\n",
        ")\n",
        "\n",
        "start_time = time()\n",
        "X_train_char = char_vectorizer.fit_transform(X_train_texts)\n",
        "X_test_char = char_vectorizer.transform(X_test_texts)\n",
        "print(f\"‚úì Character features created in {time() - start_time:.2f} seconds\")\n",
        "print(f\"  Training shape: {X_train_char.shape}\")\n",
        "\n",
        "# Combine features\n",
        "X_train = hstack([X_train_word, X_train_char])\n",
        "X_test = hstack([X_test_word, X_test_char])\n",
        "\n",
        "print(f\"\\n‚úì Combined feature matrix:\")\n",
        "print(f\"  Training shape: {X_train.shape}\")\n",
        "print(f\"  Test shape: {X_test.shape}\")\n",
        "print(f\"  Total features: {X_train.shape[1]:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression baseline\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING LOGISTIC REGRESSION (BASELINE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    random_state=RANDOM_STATE,\n",
        "    C=1.0,\n",
        "    solver='lbfgs',\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "start_time = time()\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_train_time = time() - start_time\n",
        "\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"‚úì Training completed in {lr_train_time:.2f} seconds\")\n",
        "print(f\"  Accuracy: {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score: {lr_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for SVM (OPTIMIZED FOR SPEED)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"HYPERPARAMETER TUNING FOR SVM\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# FAST MODE: Optimized for speed while maintaining 99%+ accuracy\n",
        "print(\"üöÄ FAST MODE ENABLED - Optimized for speed (9 fits vs 70 in original)\")\n",
        "\n",
        "cv_folds = 3  # Reduced from 5 to 3 folds (still robust, 40% faster)\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Smaller sample for faster grid search\n",
        "if X_train.shape[0] > 3000:\n",
        "    sample_size = 3000  # Reduced from 10000 for much faster training\n",
        "    sample_indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
        "    X_train_sample = X_train[sample_indices]\n",
        "    y_train_sample = y_train[sample_indices]\n",
        "    print(f\"Using {sample_size:,} samples for grid search (fast mode)...\")\n",
        "else:\n",
        "    X_train_sample = X_train\n",
        "    y_train_sample = y_train\n",
        "\n",
        "# Optimized parameter grid - focuses on most effective values\n",
        "param_grid = {\n",
        "    'C': [1.0, 2.0, 5.0],  # Only 3 values (most effective range)\n",
        "    'class_weight': ['balanced']  # Only balanced (usually best)\n",
        "}\n",
        "\n",
        "print(f\"Grid search parameters: {param_grid}\")\n",
        "print(f\"CV folds: {cv_folds}\")\n",
        "print(f\"Total fits: {len(param_grid['C']) * len(param_grid['class_weight']) * cv_folds} (vs 70 in original)\")\n",
        "\n",
        "svm_base = SVC(\n",
        "    kernel='linear',\n",
        "    probability=True,\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=10000\n",
        ")\n",
        "\n",
        "print(\"\\nRunning grid search (optimized - should take 8-15 minutes)...\")\n",
        "start_time = time()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    svm_base,\n",
        "    param_grid,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_sample, y_train_sample)\n",
        "grid_time = time() - start_time\n",
        "\n",
        "print(f\"\\n‚úì Grid search completed in {grid_time/60:.2f} minutes\")\n",
        "print(f\"  Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"  Best CV score: {grid_search.best_score_:.4f} ({grid_search.best_score_*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final SVM model\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING FINAL SVM MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Using best parameters: {best_params}\")\n",
        "\n",
        "final_svm = SVC(\n",
        "    kernel='linear',\n",
        "    C=best_params['C'],\n",
        "    class_weight=best_params['class_weight'],\n",
        "    probability=True,\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=10000\n",
        ")\n",
        "\n",
        "print(\"Training on full training set...\")\n",
        "print(\"‚ö†Ô∏è  This may take 15-25 minutes on CPU with 50K features...\")\n",
        "start_time = time()\n",
        "final_svm.fit(X_train, y_train)\n",
        "svm_train_time = time() - start_time\n",
        "\n",
        "y_pred_svm = final_svm.predict(X_test)\n",
        "y_proba_svm = final_svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"‚úì Training completed in {svm_train_time:.2f} seconds\")\n",
        "print(f\"\\n--- Test Set Results ---\")\n",
        "print(f\"  Accuracy: {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score: {svm_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    y_test, y_pred_svm, average=None, labels=[0, 1]\n",
        ")\n",
        "\n",
        "print(f\"\\nPer-Class Metrics:\")\n",
        "print(f\"  Human (Class 0): Precision={precision[0]:.4f}, Recall={recall[0]:.4f}, F1={f1[0]:.4f}\")\n",
        "print(f\"  AI (Class 1): Precision={precision[1]:.4f}, Recall={recall[1]:.4f}, F1={f1[1]:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  TN (Human‚ÜíHuman): {cm[0][0]:,}\")\n",
        "print(f\"  FP (Human‚ÜíAI):   {cm[0][1]:,}\")\n",
        "print(f\"  FN (AI‚ÜíHuman):   {cm[1][0]:,}\")\n",
        "print(f\"  TP (AI‚ÜíAI):      {cm[1][1]:,}\")\n",
        "\n",
        "# Cross-validation (OPTIMIZED - using smaller sample for speed)\n",
        "print(\"\\nRunning cross-validation (optimized for speed)...\")\n",
        "# Use smaller sample for CV to speed up (still gives good estimate)\n",
        "if X_train.shape[0] > 5000:\n",
        "    cv_sample_size = 5000\n",
        "    cv_indices = np.random.choice(X_train.shape[0], cv_sample_size, replace=False)\n",
        "    X_train_cv = X_train[cv_indices]\n",
        "    y_train_cv = y_train[cv_indices]\n",
        "    print(f\"Using {cv_sample_size:,} samples for CV (for speed)...\")\n",
        "else:\n",
        "    X_train_cv = X_train\n",
        "    y_train_cv = y_train\n",
        "\n",
        "# Use 3-fold CV instead of 5 for faster evaluation\n",
        "cv_folds_fast = 3\n",
        "skf_fast = StratifiedKFold(n_splits=cv_folds_fast, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    final_svm, \n",
        "    X_train_cv, \n",
        "    y_train_cv, \n",
        "    cv=skf_fast, \n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"Cross-Validation Results ({cv_folds_fast}-fold on {len(X_train_cv):,} samples):\")\n",
        "print(f\"  Mean: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
        "print(f\"  Std: {cv_scores.std():.4f}\")\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'],\n",
        "            ax=axes[0], cbar_kws={'label': 'Normalized Count'})\n",
        "axes[0].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_svm)\n",
        "roc_auc = roc_auc_score(y_test, y_proba_svm)\n",
        "axes[1].plot(fpr, tpr, color='#3498db', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "axes[1].plot([0, 1], [0, 1], color='#e74c3c', lw=2, linestyle='--', label='Random')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc=\"lower right\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if svm_accuracy >= 0.99:\n",
        "    print(f\"\\nüéâ SUCCESS! Achieved 99%+ accuracy: {svm_accuracy*100:.2f}%\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Current accuracy: {svm_accuracy*100:.2f}% (Target: 99%+)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save and Download Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models\n",
        "print(\"=\" * 70)\n",
        "print(\"SAVING MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save vectorizers\n",
        "with open('models/word_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(word_vectorizer, f)\n",
        "print(\"‚úì Saved word_vectorizer.pkl\")\n",
        "\n",
        "with open('models/char_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(char_vectorizer, f)\n",
        "print(\"‚úì Saved char_vectorizer.pkl\")\n",
        "\n",
        "# Save models\n",
        "with open('models/svm_model.pkl', 'wb') as f:\n",
        "    pickle.dump(final_svm, f)\n",
        "print(\"‚úì Saved svm_model.pkl\")\n",
        "\n",
        "with open('models/logreg_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lr_model, f)\n",
        "print(\"‚úì Saved logreg_model.pkl\")\n",
        "\n",
        "# Save label encoder\n",
        "with open('models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "print(\"‚úì Saved label_encoder.pkl\")\n",
        "\n",
        "print(\"\\n‚úì All models saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download models\n",
        "print(\"=\" * 70)\n",
        "print(\"DOWNLOADING MODELS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nDownloading model files...\")\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Create zip file with all models\n",
        "with zipfile.ZipFile('ai_text_detector_models.zip', 'w') as zipf:\n",
        "    zipf.write('models/word_vectorizer.pkl')\n",
        "    zipf.write('models/char_vectorizer.pkl')\n",
        "    zipf.write('models/svm_model.pkl')\n",
        "    zipf.write('models/logreg_model.pkl')\n",
        "    zipf.write('models/label_encoder.pkl')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('ai_text_detector_models.zip')\n",
        "\n",
        "print(\"‚úì Models downloaded! Extract the zip file to use the models.\")\n",
        "print(\"\\nModel files:\")\n",
        "print(\"  - word_vectorizer.pkl\")\n",
        "print(\"  - char_vectorizer.pkl\")\n",
        "print(\"  - svm_model.pkl\")\n",
        "print(\"  - logreg_model.pkl\")\n",
        "print(\"  - label_encoder.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"  Total samples: {len(balanced_dataset):,}\")\n",
        "print(f\"  Training: {len(X_train_texts):,}, Test: {len(X_test_texts):,}\")\n",
        "\n",
        "print(f\"\\nüîß Features:\")\n",
        "print(f\"  Word: {X_train_word.shape[1]:,}, Char: {X_train_char.shape[1]:,}\")\n",
        "print(f\"  Total: {X_train.shape[1]:,}\")\n",
        "\n",
        "print(f\"\\nüéØ Model Performance:\")\n",
        "print(f\"  SVM Accuracy: {svm_accuracy*100:.2f}%\")\n",
        "print(f\"  SVM F1-Score: {svm_f1:.4f}\")\n",
        "print(f\"  Logistic Regression Accuracy: {lr_accuracy*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ Cross-Validation:\")\n",
        "print(f\"  Mean: {cv_scores.mean()*100:.2f}%\")\n",
        "print(f\"  Std: {cv_scores.std():.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ Models:\")\n",
        "print(f\"  All models saved and ready for download!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
