{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced AI Text Detection - Complete Training Pipeline\n",
        "\n",
        "This notebook implements a comprehensive training pipeline for AI text detection, optimized to achieve 99%+ accuracy.\n",
        "\n",
        "**Features:**\n",
        "- Advanced preprocessing using project utilities\n",
        "- Optimized feature engineering (word + character n-grams)\n",
        "- Extensive hyperparameter tuning\n",
        "- Cross-validation for robust evaluation\n",
        "- Multiple model comparison\n",
        "- Comprehensive visualizations\n",
        "- Detailed performance metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend for headless execution\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.ioff()  # Turn off interactive mode\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from collections import Counter\n",
        "from time import time\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, \n",
        "    GridSearchCV, \n",
        "    cross_val_score,\n",
        "    StratifiedKFold,\n",
        "    learning_curve\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    f1_score\n",
        ")\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Add project paths - fix path to work from notebooks directory\n",
        "notebook_dir = Path().absolute()\n",
        "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import project utilities\n",
        "from preprocessing.advanced_preprocessing import clean_text, preprocess_article\n",
        "from utils.text_utils import get_text_statistics\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(\"‚úì All libraries imported successfully!\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the combined dataset\n",
        "dataset_path = project_root / \"data\" / \"combined\" / \"combined_dataset_clean.json\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"LOADING DATASET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if not dataset_path.exists():\n",
        "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "with open(str(dataset_path), \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(f\"‚úì Loaded {len(dataset):,} articles from {dataset_path}\")\n",
        "\n",
        "# Display sample\n",
        "if len(dataset) > 0:\n",
        "    print(\"\\nSample article:\")\n",
        "    print(json.dumps(dataset[0], indent=2, ensure_ascii=False))\n",
        "else:\n",
        "    raise ValueError(\"Dataset is empty!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze dataset distribution\n",
        "labels = [article.get(\"label\", \"\").lower() for article in dataset]\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATASET ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nLabel Distribution:\")\n",
        "for label, count in label_counts.items():\n",
        "    percentage = (count / len(dataset)) * 100\n",
        "    print(f\"  {label.upper()}: {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate text statistics\n",
        "text_lengths = []\n",
        "word_counts = []\n",
        "for article in dataset:\n",
        "    content = article.get(\"content\", \"\")\n",
        "    text_lengths.append(len(content))\n",
        "    word_counts.append(len(content.split()))\n",
        "\n",
        "print(f\"\\nText Statistics:\")\n",
        "print(f\"  Average length: {np.mean(text_lengths):.0f} characters\")\n",
        "print(f\"  Average words: {np.mean(word_counts):.0f} words\")\n",
        "print(f\"  Min length: {np.min(text_lengths)} characters\")\n",
        "print(f\"  Max length: {np.max(text_lengths):,} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dataset distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Label distribution\n",
        "axes[0].bar(label_counts.keys(), label_counts.values(), color=['#3498db', '#e74c3c'])\n",
        "axes[0].set_title('Label Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Label')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for i, (label, count) in enumerate(label_counts.items()):\n",
        "    axes[0].text(i, count, f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Text length distribution\n",
        "axes[1].hist(text_lengths, bins=50, color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "axes[1].set_title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Character Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Word count distribution\n",
        "axes[2].hist(word_counts, bins=50, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
        "axes[2].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
        "axes[2].set_xlabel('Word Count')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Advanced Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess dataset using advanced preprocessing\n",
        "print(\"=\" * 70)\n",
        "print(\"PREPROCESSING DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Normalize labels and clean texts\n",
        "processed_articles = []\n",
        "empty_count = 0\n",
        "\n",
        "for article in dataset:\n",
        "    # Normalize label\n",
        "    label = article.get(\"label\", \"\").lower()\n",
        "    if label not in [\"ai\", \"human\"]:\n",
        "        continue\n",
        "    \n",
        "    # Get and clean content\n",
        "    content = article.get(\"content\", \"\").strip()\n",
        "    if not content:\n",
        "        empty_count += 1\n",
        "        continue\n",
        "    \n",
        "    # Apply advanced cleaning\n",
        "    cleaned_content = clean_text(content)\n",
        "    \n",
        "    if cleaned_content and len(cleaned_content) > 20:  # Minimum length threshold\n",
        "        processed_articles.append({\n",
        "            \"content\": cleaned_content,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "print(f\"‚úì Processed {len(processed_articles):,} articles\")\n",
        "print(f\"  Removed {len(dataset) - len(processed_articles):,} invalid/empty articles\")\n",
        "print(f\"  Empty content count: {empty_count}\")\n",
        "\n",
        "# Balance dataset\n",
        "ai_articles = [a for a in processed_articles if a[\"label\"] == \"ai\"]\n",
        "human_articles = [a for a in processed_articles if a[\"label\"] == \"human\"]\n",
        "\n",
        "print(f\"\\nBefore balancing:\")\n",
        "print(f\"  AI articles: {len(ai_articles):,}\")\n",
        "print(f\"  Human articles: {len(human_articles):,}\")\n",
        "\n",
        "# Balance to minimum class size\n",
        "min_size = min(len(ai_articles), len(human_articles))\n",
        "ai_articles = ai_articles[:min_size]\n",
        "human_articles = human_articles[:min_size]\n",
        "\n",
        "balanced_dataset = ai_articles + human_articles\n",
        "balanced_dataset = shuffle(balanced_dataset, random_state=RANDOM_STATE)\n",
        "\n",
        "print(f\"\\nAfter balancing:\")\n",
        "print(f\"  AI articles: {len(ai_articles):,}\")\n",
        "print(f\"  Human articles: {len(human_articles):,}\")\n",
        "print(f\"  Total: {len(balanced_dataset):,}\")\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = [article[\"content\"] for article in balanced_dataset]\n",
        "labels = [article[\"label\"] for article in balanced_dataset]\n",
        "\n",
        "if len(texts) == 0:\n",
        "    raise ValueError(\"No valid texts after preprocessing!\")\n",
        "\n",
        "print(f\"\\n‚úì Final dataset: {len(texts):,} samples ready for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Advanced Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nLabel encoding:\")\n",
        "print(f\"  Classes: {label_encoder.classes_}\")\n",
        "print(f\"  Distribution: AI={np.sum(y==1):,}, Human={np.sum(y==0):,}\")\n",
        "\n",
        "# Split data first (before feature extraction to avoid data leakage)\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    texts, y, \n",
        "    test_size=0.2, \n",
        "    random_state=RANDOM_STATE, \n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain/Test Split:\")\n",
        "print(f\"  Training: {len(X_train_texts):,} samples\")\n",
        "print(f\"  Testing: {len(X_test_texts):,} samples\")\n",
        "print(f\"  Train labels - AI: {np.sum(y_train==1):,}, Human: {np.sum(y_train==0):,}\")\n",
        "print(f\"  Test labels - AI: {np.sum(y_test==1):,}, Human: {np.sum(y_test==0):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimized word-level TF-IDF features\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Creating Word-Level Features...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    max_features=25000,      # Increased for better accuracy\n",
        "    ngram_range=(1, 3),       # Unigrams, bigrams, trigrams\n",
        "    sublinear_tf=True,        # Apply sublinear TF scaling (log scale)\n",
        "    min_df=2,                 # Minimum document frequency\n",
        "    max_df=0.95,              # Maximum document frequency (remove very common words)\n",
        "    analyzer='word',\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode',\n",
        "    token_pattern=r'(?u)\\b\\w+\\b'  # Better tokenization\n",
        ")\n",
        "\n",
        "print(\"Fitting word vectorizer on training data...\")\n",
        "start_time = time()\n",
        "X_train_word = word_vectorizer.fit_transform(X_train_texts)\n",
        "X_test_word = word_vectorizer.transform(X_test_texts)\n",
        "print(f\"‚úì Word features created in {time() - start_time:.2f} seconds\")\n",
        "print(f\"  Training shape: {X_train_word.shape}\")\n",
        "print(f\"  Test shape: {X_test_word.shape}\")\n",
        "print(f\"  Vocabulary size: {len(word_vectorizer.vocabulary_):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimized character-level TF-IDF features\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Creating Character-Level Features...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3, 6),       # 3-6 character n-grams (critical for AI detection)\n",
        "    max_features=35000,       # Increased for better pattern detection\n",
        "    sublinear_tf=True,\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode'\n",
        ")\n",
        "\n",
        "print(\"Fitting character vectorizer on training data...\")\n",
        "start_time = time()\n",
        "X_train_char = char_vectorizer.fit_transform(X_train_texts)\n",
        "X_test_char = char_vectorizer.transform(X_test_texts)\n",
        "print(f\"‚úì Character features created in {time() - start_time:.2f} seconds\")\n",
        "print(f\"  Training shape: {X_train_char.shape}\")\n",
        "print(f\"  Test shape: {X_test_char.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine word and character features\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Combining Features...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "X_train = hstack([X_train_word, X_train_char])\n",
        "X_test = hstack([X_test_word, X_test_char])\n",
        "\n",
        "print(f\"‚úì Combined feature matrix created\")\n",
        "print(f\"  Training shape: {X_train.shape}\")\n",
        "print(f\"  Test shape: {X_test.shape}\")\n",
        "print(f\"  Total features: {X_train.shape[1]:,}\")\n",
        "print(f\"  Sparsity: {(1 - X_train.nnz / (X_train.shape[0] * X_train.shape[1])) * 100:.2f}%\")\n",
        "print(f\"  Memory usage: ~{X_train.data.nbytes / (1024**2):.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training with Hyperparameter Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression as baseline\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING LOGISTIC REGRESSION (BASELINE)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    random_state=RANDOM_STATE,\n",
        "    C=1.0,\n",
        "    solver='lbfgs',\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'  # Handle class imbalance\n",
        ")\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "start_time = time()\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_train_time = time() - start_time\n",
        "\n",
        "# Evaluate\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"‚úì Training completed in {lr_train_time:.2f} seconds\")\n",
        "print(f\"  Accuracy: {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score: {lr_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for SVM (optimized for 99% accuracy)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"HYPERPARAMETER TUNING FOR SVM\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Use stratified K-fold for better validation\n",
        "cv_folds = 5\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Sample data for faster grid search (if dataset is large)\n",
        "if X_train.shape[0] > 10000:\n",
        "    sample_size = 10000\n",
        "    sample_indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
        "    X_train_sample = X_train[sample_indices]\n",
        "    y_train_sample = y_train[sample_indices]\n",
        "    print(f\"Using {sample_size:,} samples for grid search (for speed)...\")\n",
        "else:\n",
        "    X_train_sample = X_train\n",
        "    y_train_sample = y_train\n",
        "\n",
        "# Extended parameter grid for better accuracy\n",
        "param_grid = {\n",
        "    'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0],  # Wider range\n",
        "    'class_weight': [None, 'balanced']  # Try both\n",
        "}\n",
        "\n",
        "print(f\"\\nGrid search parameters:\")\n",
        "print(f\"  C values: {param_grid['C']}\")\n",
        "print(f\"  Class weights: {param_grid['class_weight']}\")\n",
        "print(f\"  CV folds: {cv_folds}\")\n",
        "\n",
        "# Create SVM with linear kernel\n",
        "svm_base = SVC(\n",
        "    kernel='linear',\n",
        "    probability=True,\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=10000  # Increased for convergence\n",
        ")\n",
        "\n",
        "print(\"\\nRunning grid search (this may take several minutes)...\")\n",
        "start_time = time()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    svm_base,\n",
        "    param_grid,\n",
        "    cv=skf,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "grid_time = time() - start_time\n",
        "\n",
        "print(f\"\\n‚úì Grid search completed in {grid_time/60:.2f} minutes\")\n",
        "print(f\"  Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"  Best CV score: {grid_search.best_score_:.4f} ({grid_search.best_score_*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final SVM model on full training set with best parameters\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING FINAL SVM MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Using best parameters: {best_params}\")\n",
        "\n",
        "final_svm = SVC(\n",
        "    kernel='linear',\n",
        "    C=best_params['C'],\n",
        "    class_weight=best_params['class_weight'],\n",
        "    probability=True,\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=10000\n",
        ")\n",
        "\n",
        "print(\"Training on full training set...\")\n",
        "start_time = time()\n",
        "final_svm.fit(X_train, y_train)\n",
        "svm_train_time = time() - start_time\n",
        "\n",
        "print(f\"‚úì Training completed in {svm_train_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred_svm = final_svm.predict(X_test)\n",
        "y_proba_svm = final_svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"\\n--- Test Set Results ---\")\n",
        "print(f\"  Accuracy: {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-Score: {svm_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try additional models for comparison\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING ADDITIONAL MODELS FOR COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "models = {}\n",
        "results = {}\n",
        "\n",
        "# Random Forest\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=50,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "start_time = time()\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_time = time() - start_time\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "models['Random Forest'] = rf_model\n",
        "results['Random Forest'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'f1': f1_score(y_test, y_pred_rf),\n",
        "    'time': rf_time\n",
        "}\n",
        "print(f\"  Accuracy: {results['Random Forest']['accuracy']:.4f} ({results['Random Forest']['accuracy']*100:.2f}%)\")\n",
        "\n",
        "# Gradient Boosting\n",
        "print(\"\\nTraining Gradient Boosting...\")\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=0\n",
        ")\n",
        "start_time = time()\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_time = time() - start_time\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "models['Gradient Boosting'] = gb_model\n",
        "results['Gradient Boosting'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_gb),\n",
        "    'f1': f1_score(y_test, y_pred_gb),\n",
        "    'time': gb_time\n",
        "}\n",
        "print(f\"  Accuracy: {results['Gradient Boosting']['accuracy']:.4f} ({results['Gradient Boosting']['accuracy']*100:.2f}%)\")\n",
        "\n",
        "# Add SVM and LR to results\n",
        "models['SVM'] = final_svm\n",
        "models['Logistic Regression'] = lr_model\n",
        "results['SVM'] = {'accuracy': svm_accuracy, 'f1': svm_f1, 'time': svm_train_time}\n",
        "results['Logistic Regression'] = {'accuracy': lr_accuracy, 'f1': lr_f1, 'time': lr_train_time}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comprehensive Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation of best model (SVM)\n",
        "print(\"=\" * 70)\n",
        "print(\"DETAILED EVALUATION - SVM MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate all metrics\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    y_test, y_pred_svm, average=None, labels=[0, 1]\n",
        ")\n",
        "\n",
        "print(f\"\\nPer-Class Metrics:\")\n",
        "print(f\"  Human (Class 0):\")\n",
        "print(f\"    Precision: {precision[0]:.4f}\")\n",
        "print(f\"    Recall: {recall[0]:.4f}\")\n",
        "print(f\"    F1-Score: {f1[0]:.4f}\")\n",
        "print(f\"    Support: {support[0]:,}\")\n",
        "print(f\"  AI (Class 1):\")\n",
        "print(f\"    Precision: {precision[1]:.4f}\")\n",
        "print(f\"    Recall: {recall[1]:.4f}\")\n",
        "print(f\"    F1-Score: {f1[1]:.4f}\")\n",
        "print(f\"    Support: {support[1]:,}\")\n",
        "\n",
        "# Weighted averages\n",
        "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
        "    y_test, y_pred_svm, average='weighted'\n",
        ")\n",
        "\n",
        "print(f\"\\nWeighted Averages:\")\n",
        "print(f\"  Precision: {weighted_precision:.4f}\")\n",
        "print(f\"  Recall: {weighted_recall:.4f}\")\n",
        "print(f\"  F1-Score: {weighted_f1:.4f}\")\n",
        "\n",
        "# ROC-AUC\n",
        "try:\n",
        "    roc_auc = roc_auc_score(y_test, y_proba_svm)\n",
        "    print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"  ROC-AUC: Could not calculate ({e})\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  True Negatives (Human‚ÜíHuman):  {cm[0][0]:,}\")\n",
        "print(f\"  False Positives (Human‚ÜíAI):    {cm[0][1]:,}\")\n",
        "print(f\"  False Negatives (AI‚ÜíHuman):    {cm[1][0]:,}\")\n",
        "print(f\"  True Positives (AI‚ÜíAI):        {cm[1][1]:,}\")\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=['Human', 'AI']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation on full training set\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CROSS-VALIDATION EVALUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"Running 5-fold cross-validation on training set...\")\n",
        "cv_scores = cross_val_score(\n",
        "    final_svm, \n",
        "    X_train, \n",
        "    y_train, \n",
        "    cv=skf, \n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"\\nCross-Validation Results:\")\n",
        "print(f\"  Scores: {cv_scores}\")\n",
        "print(f\"  Mean: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
        "print(f\"  Std: {cv_scores.std():.4f}\")\n",
        "print(f\"  95% CI: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, {cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Model Accuracy Comparison\n",
        "model_names = list(results.keys())\n",
        "accuracies = [results[m]['accuracy'] * 100 for m in model_names]\n",
        "f1_scores = [results[m]['f1'] * 100 for m in model_names]\n",
        "times = [results[m]['time'] for m in model_names]\n",
        "\n",
        "axes[0, 0].barh(model_names, accuracies, color=['#3498db', '#e74c3c', '#2ecc71', '#9b59b6'])\n",
        "axes[0, 0].set_xlabel('Accuracy (%)', fontsize=12)\n",
        "axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "for i, acc in enumerate(accuracies):\n",
        "    axes[0, 0].text(acc, i, f'{acc:.2f}%', va='center', ha='left', fontweight='bold')\n",
        "\n",
        "# 2. F1-Score Comparison\n",
        "axes[0, 1].barh(model_names, f1_scores, color=['#3498db', '#e74c3c', '#2ecc71', '#9b59b6'])\n",
        "axes[0, 1].set_xlabel('F1-Score (%)', fontsize=12)\n",
        "axes[0, 1].set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "for i, f1 in enumerate(f1_scores):\n",
        "    axes[0, 1].text(f1, i, f'{f1:.2f}%', va='center', ha='left', fontweight='bold')\n",
        "\n",
        "# 3. Training Time Comparison\n",
        "axes[1, 0].barh(model_names, times, color=['#3498db', '#e74c3c', '#2ecc71', '#9b59b6'])\n",
        "axes[1, 0].set_xlabel('Training Time (seconds)', fontsize=12)\n",
        "axes[1, 0].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(axis='x', alpha=0.3)\n",
        "for i, t in enumerate(times):\n",
        "    axes[1, 0].text(t, i, f'{t:.2f}s', va='center', ha='left', fontweight='bold')\n",
        "\n",
        "# 4. Confusion Matrix Heatmap\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
        "            xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'],\n",
        "            ax=axes[1, 1], cbar_kws={'label': 'Normalized Count'})\n",
        "axes[1, 1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('True Label', fontsize=12)\n",
        "axes[1, 1].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Best Model: SVM with {svm_accuracy*100:.2f}% accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve and Precision-Recall Curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_svm)\n",
        "roc_auc = roc_auc_score(y_test, y_proba_svm)\n",
        "\n",
        "axes[0].plot(fpr, tpr, color='#3498db', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "axes[0].plot([0, 1], [0, 1], color='#e74c3c', lw=2, linestyle='--', label='Random')\n",
        "axes[0].set_xlim([0.0, 1.0])\n",
        "axes[0].set_ylim([0.0, 1.05])\n",
        "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(loc=\"lower right\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba_svm)\n",
        "\n",
        "axes[1].plot(recall_curve, precision_curve, color='#2ecc71', lw=2, label='Precision-Recall curve')\n",
        "axes[1].set_xlabel('Recall', fontsize=12)\n",
        "axes[1].set_ylabel('Precision', fontsize=12)\n",
        "axes[1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc=\"lower left\")\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Learning Curve\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GENERATING LEARNING CURVE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    final_svm, X_train, y_train,\n",
        "    cv=skf, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_sizes, train_mean, 'o-', color='#3498db', label='Training Score')\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='#3498db')\n",
        "plt.plot(train_sizes, val_mean, 'o-', color='#e74c3c', label='Validation Score')\n",
        "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='#e74c3c')\n",
        "plt.xlabel('Training Set Size', fontsize=12)\n",
        "plt.ylabel('Accuracy Score', fontsize=12)\n",
        "plt.title('Learning Curve - SVM Model', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Learning curve generated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Models and Vectorizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all models and vectorizers\n",
        "print(\"=\" * 70)\n",
        "print(\"SAVING MODELS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "model_save_dir = project_root / \"models\" / \"saved_models\"\n",
        "os.makedirs(str(model_save_dir), exist_ok=True)\n",
        "\n",
        "# Save vectorizers\n",
        "with open(str(model_save_dir / \"word_vectorizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(word_vectorizer, f)\n",
        "print(\"‚úì Saved word_vectorizer.pkl\")\n",
        "\n",
        "with open(str(model_save_dir / \"char_vectorizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(char_vectorizer, f)\n",
        "print(\"‚úì Saved char_vectorizer.pkl\")\n",
        "\n",
        "# Save models\n",
        "with open(str(model_save_dir / \"svm_model.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(final_svm, f)\n",
        "print(\"‚úì Saved svm_model.pkl\")\n",
        "\n",
        "with open(str(model_save_dir / \"logreg_model.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(lr_model, f)\n",
        "print(\"‚úì Saved logreg_model.pkl\")\n",
        "\n",
        "# Save label encoder\n",
        "with open(str(model_save_dir / \"label_encoder.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "print(\"‚úì Saved label_encoder.pkl\")\n",
        "\n",
        "print(f\"\\n‚úì All models saved to: {model_save_dir}\")\n",
        "\n",
        "# Display file sizes\n",
        "print(\"\\nModel file sizes:\")\n",
        "for filename in [\"word_vectorizer.pkl\", \"char_vectorizer.pkl\", \"svm_model.pkl\", \"logreg_model.pkl\", \"label_encoder.pkl\"]:\n",
        "    filepath = model_save_dir / filename\n",
        "    if filepath.exists():\n",
        "        size_mb = filepath.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  {filename}: {size_mb:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Predictions on Sample Texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test predictions on sample texts\n",
        "print(\"=\" * 70)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get some test samples\n",
        "test_samples = [\n",
        "    X_test_texts[0],\n",
        "    X_test_texts[1],\n",
        "    X_test_texts[2],\n",
        "    X_test_texts[3],\n",
        "    X_test_texts[4]\n",
        "]\n",
        "\n",
        "true_labels = [label_encoder.inverse_transform([y_test[i]])[0] for i in range(5)]\n",
        "\n",
        "for i, (text, true_label) in enumerate(zip(test_samples, true_labels)):\n",
        "    # Create features\n",
        "    X_word_sample = word_vectorizer.transform([text])\n",
        "    X_char_sample = char_vectorizer.transform([text])\n",
        "    X_sample = hstack([X_word_sample, X_char_sample])\n",
        "    \n",
        "    # Predict\n",
        "    prediction = final_svm.predict(X_sample)[0]\n",
        "    probabilities = final_svm.predict_proba(X_sample)[0]\n",
        "    predicted_label = label_encoder.inverse_transform([prediction])[0]\n",
        "    confidence = probabilities[prediction] * 100\n",
        "    \n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"True Label: {true_label.upper()}\")\n",
        "    print(f\"Predicted: {predicted_label.upper()} (Confidence: {confidence:.2f}%)\")\n",
        "    print(f\"Probabilities - Human: {probabilities[0]*100:.2f}%, AI: {probabilities[1]*100:.2f}%\")\n",
        "    print(f\"Text preview: {text[:150]}...\")\n",
        "    print(f\"‚úì Correct!\" if true_label == predicted_label else f\"‚úó Incorrect!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä Dataset:\")\n",
        "print(f\"  Total samples: {len(balanced_dataset):,}\")\n",
        "print(f\"  Training samples: {len(X_train_texts):,}\")\n",
        "print(f\"  Test samples: {len(X_test_texts):,}\")\n",
        "print(f\"  Balanced classes: {len(ai_articles):,} AI, {len(human_articles):,} Human\")\n",
        "\n",
        "print(f\"\\nüîß Features:\")\n",
        "print(f\"  Word features: {X_train_word.shape[1]:,}\")\n",
        "print(f\"  Character features: {X_train_char.shape[1]:,}\")\n",
        "print(f\"  Total features: {X_train.shape[1]:,}\")\n",
        "\n",
        "print(f\"\\nüéØ Model Performance (Test Set):\")\n",
        "print(f\"  SVM Accuracy: {svm_accuracy*100:.2f}%\")\n",
        "print(f\"  SVM F1-Score: {svm_f1:.4f}\")\n",
        "print(f\"  Logistic Regression Accuracy: {lr_accuracy*100:.2f}%\")\n",
        "print(f\"  Random Forest Accuracy: {results['Random Forest']['accuracy']*100:.2f}%\")\n",
        "print(f\"  Gradient Boosting Accuracy: {results['Gradient Boosting']['accuracy']*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ Cross-Validation:\")\n",
        "print(f\"  Mean CV Accuracy: {cv_scores.mean()*100:.2f}%\")\n",
        "print(f\"  Std: {cv_scores.std():.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ Models Saved:\")\n",
        "print(f\"  Location: {str(model_save_dir)}\")\n",
        "print(f\"  Files: word_vectorizer.pkl, char_vectorizer.pkl, svm_model.pkl, logreg_model.pkl, label_encoder.pkl\")\n",
        "\n",
        "if svm_accuracy >= 0.99:\n",
        "    print(f\"\\nüéâ SUCCESS! Achieved 99%+ accuracy: {svm_accuracy*100:.2f}%\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Current accuracy: {svm_accuracy*100:.2f}% (Target: 99%+)\")\n",
        "    print(f\"   Consider: increasing max_features, tuning hyperparameters further, or adding more data\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
